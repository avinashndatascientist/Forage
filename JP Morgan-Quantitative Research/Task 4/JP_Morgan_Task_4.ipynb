{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LSkp2AakCK9",
        "outputId": "1f894a5e-d567-4549-b4da-fbd83fa9137c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Number of Buckets: 1\n",
            "Optimal Bucket Boundaries: [425. 831.]\n",
            "Optimal Number of Buckets (Log-Likelihood): 3\n",
            "Optimal Bucket Boundaries (Log-Likelihood): [425.         613.         664.66666667 831.        ]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = pd.read_csv('Task 3 and 4_Loan_Data.csv')\n",
        "\n",
        "X = data.drop(['customer_id', 'default'], axis=1)\n",
        "y = data['default']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict default probabilities on the test set\n",
        "default_probs = logreg_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Get FICO scores from the test set\n",
        "fico_scores = X_test['fico_score']\n",
        "\n",
        "# Calculate the mean squared error for different bucket boundaries\n",
        "mse_values = []\n",
        "num_buckets = 5  # You can adjust the number of buckets\n",
        "\n",
        "for i in range(1, num_buckets):\n",
        "    bucket_boundaries = np.percentile(fico_scores, np.linspace(0, 100, i + 1))\n",
        "    bucket_predictions = [default_probs[(fico_scores >= bucket_boundaries[j]) & (fico_scores <= bucket_boundaries[j + 1])].mean() for j in range(i)]\n",
        "    mse = mean_squared_error(bucket_predictions, y_test[:len(bucket_predictions)])  # Ensure lengths match\n",
        "    mse_values.append((i, mse))\n",
        "\n",
        "# Find the optimal number of buckets with minimum MSE\n",
        "optimal_num_buckets = min(mse_values, key=lambda x: x[1])[0]\n",
        "print(\"Optimal Number of Buckets:\", optimal_num_buckets)\n",
        "\n",
        "# Calculate bucket boundaries based on the optimal number of buckets\n",
        "optimal_bucket_boundaries = np.percentile(fico_scores, np.linspace(0, 100, optimal_num_buckets + 1))\n",
        "\n",
        "print(\"Optimal Bucket Boundaries:\", optimal_bucket_boundaries)\n",
        "\n",
        "# Calculate log-likelihood values for different bucket boundaries\n",
        "log_likelihood_values = []\n",
        "num_buckets = 5  # You can adjust the number of buckets\n",
        "\n",
        "for i in range(1, num_buckets):\n",
        "    bucket_boundaries = np.percentile(fico_scores, np.linspace(0, 100, i + 1))\n",
        "    bucket_indices = np.digitize(fico_scores, bucket_boundaries) - 1\n",
        "\n",
        "    bucket_probabilities = [default_probs[bucket_indices == j].mean() for j in range(i)]\n",
        "    bucket_defaults = [y_test[bucket_indices == j].sum() for j in range(i)]\n",
        "    bucket_total = [len(y_test[bucket_indices == j]) for j in range(i)]\n",
        "    bucket_likelihood = sum([(default * np.log(prob) + (total - default) * np.log(1 - prob)) for default, prob, total in zip(bucket_defaults, bucket_probabilities, bucket_total)])\n",
        "\n",
        "    log_likelihood_values.append((i, bucket_likelihood))\n",
        "\n",
        "# Find the optimal number of buckets with maximum log-likelihood\n",
        "optimal_num_buckets = max(log_likelihood_values, key=lambda x: x[1])[0]\n",
        "print(\"Optimal Number of Buckets (Log-Likelihood):\", optimal_num_buckets)\n",
        "\n",
        "# Calculate bucket boundaries based on the optimal number of buckets\n",
        "optimal_bucket_boundaries = np.percentile(fico_scores, np.linspace(0, 100, optimal_num_buckets + 1))\n",
        "\n",
        "print(\"Optimal Bucket Boundaries (Log-Likelihood):\", optimal_bucket_boundaries)\n"
      ]
    }
  ]
}